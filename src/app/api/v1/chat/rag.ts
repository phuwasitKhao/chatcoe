import { openai } from "./openaiClient";
import { createCompletion, createStreamingCompletion } from "./chat";
import { Pinecone } from "@pinecone-database/pinecone";

const pinecone = new Pinecone({
    apiKey: process.env.PINECONE_API_KEY || "",
});

// üîπ ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏õ‡∏Å‡∏ï‡∏¥: ‡∏£‡∏ß‡∏° context + ‡∏ï‡∏≠‡∏ö‡∏ó‡∏µ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß
export const queryWithContext = async (
    message: string,
    modelName?: string
): Promise<string> => {
    try {
        const embeddingRes = await openai.embeddings.create({
            model: "text-embedding-3-small",
            input: message,
        });

        const queryVector = embeddingRes.data[0].embedding;

        const index = pinecone.Index(process.env.PINECONE_INDEX_NAME || "");
        const queryResult = await index.query({
            vector: queryVector,
            topK: 5,
            includeMetadata: true,
        });

        const contexts = queryResult.matches
            ?.map((match) => match.metadata?.text || "")
            .join("\n---\n");

        const prompt = `‡∏Ñ‡∏∏‡∏ì‡∏à‡∏∞‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö context ‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á ‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ô‡∏µ‡πâ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÉ‡∏´‡πâ‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô

Context:
${contexts}

‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏Ç‡∏≠‡∏á‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ:
${message}
`;

        return await createCompletion(prompt, modelName);
    } catch (err) {
        console.error("‚ùå Failed to generate answer with context:", err);
        throw err;
    }
};

// üîπ ‡πÅ‡∏ö‡∏ö streaming: ‡∏£‡∏ß‡∏° context + ‡∏ï‡∏≠‡∏ö‡πÄ‡∏õ‡πá‡∏ô chunk
export const streamWithContext = async (
    message: string,
    onChunk: (chunk: string) => void,
    modelName?: string
): Promise<void> => {
    try {
        const embeddingRes = await openai.embeddings.create({
            model: "text-embedding-3-small",
            input: message,
        });

        const queryVector = embeddingRes.data[0].embedding;

        const index = pinecone.Index(process.env.PINECONE_INDEX_NAME || "");
        const queryResult = await index.query({
            vector: queryVector,
            topK: 5,
            includeMetadata: true,
        });

        const contexts = queryResult.matches
            ?.map((match) => match.metadata?.text || "")
            .join("\n---\n");

        const prompt = `‡∏Ñ‡∏∏‡∏ì‡∏à‡∏∞‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö context ‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á ‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ô‡∏µ‡πâ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÉ‡∏´‡πâ‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô

Context:
${contexts}

‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏Ç‡∏≠‡∏á‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ:
${message}
`;

        await createStreamingCompletion(prompt, onChunk, modelName);
    } catch (err) {
        console.error("‚ùå Failed to stream answer with context:", err);
        throw err;
    }
};
